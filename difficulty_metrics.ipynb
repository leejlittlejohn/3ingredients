{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulty Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty = pd.read_csv('data/dirty_recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                         title  \\\n0              Lentil, Apple, and Turkey Wrap    \n1  Boudin Blanc Terrine with Red Onion Confit    \n2                Potato and Fennel Soup Hodge    \n3             Mahi-Mahi in Tomato Olive Sauce    \n4                    Spinach Noodle Casserole    \n\n                                              method  \\\n0  1. Place the stock, lentils, celery, carrot, t...   \n1  Combine first 9 ingredients in heavy medium sa...   \n2  In a large heavy saucepan cook diced fennel an...   \n3  Heat oil in heavy large skillet over medium-hi...   \n4  Preheat oven to 350°F. Lightly grease 8x8x2-in...   \n\n                                         ingredients  url  \n0  ['4 cups low-sodium vegetable or chicken stock...  NaN  \n1  ['1 1/2 cups whipping cream', '2 medium onions...  NaN  \n2  ['1 fennel bulb (sometimes called anise), stal...  NaN  \n3  ['2 tablespoons extra-virgin olive oil', '1 cu...  NaN  \n4  ['1 12-ounce package frozen spinach soufflé, t...  NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>method</th>\n      <th>ingredients</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lentil, Apple, and Turkey Wrap</td>\n      <td>1. Place the stock, lentils, celery, carrot, t...</td>\n      <td>['4 cups low-sodium vegetable or chicken stock...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n      <td>Combine first 9 ingredients in heavy medium sa...</td>\n      <td>['1 1/2 cups whipping cream', '2 medium onions...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Potato and Fennel Soup Hodge</td>\n      <td>In a large heavy saucepan cook diced fennel an...</td>\n      <td>['1 fennel bulb (sometimes called anise), stal...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mahi-Mahi in Tomato Olive Sauce</td>\n      <td>Heat oil in heavy large skillet over medium-hi...</td>\n      <td>['2 tablespoons extra-virgin olive oil', '1 cu...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Spinach Noodle Casserole</td>\n      <td>Preheat oven to 350°F. Lightly grease 8x8x2-in...</td>\n      <td>['1 12-ounce package frozen spinach soufflé, t...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dirty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Peel a wide strip from around the middle of each potato. In a kettle combine potatoes and garlic with enough salted water to cover by 2 inches and boil until potatoes are just tender, about 15 minutes. Drain mixture. Transfer potatoes to a bowl and transfer garlic to a blender. Add to blender lemon juice, oil, and salt and pepper to taste and purée dressing. In the bowl toss potatoes with dressing and parsley. Potatoes may be prepared 4 hours ahead and kept covered. Serve potatoes warm or at room temperature.'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# check a method example\n",
    "dirty['method'][20_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 20000/20000 [14:40<00:00, 22.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# take verbs from first 20,000 recipes\n",
    "\n",
    "verbs = []\n",
    "for row in tqdm(dirty['method'].values[:20000]):\n",
    "    doc = nlp(row)\n",
    "    for tok in doc:\n",
    "        if (tok.pos_ == 'VERB'):\n",
    "            if tok.dep_ == 'ROOT':\n",
    "                verbs.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'conj':\n",
    "                verbs.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'xcomp':\n",
    "                verbs.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'nsubj':\n",
    "                verbs.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'det':\n",
    "                verbs.append(tok.lemma_.lower())\n",
    "\n",
    "verbs = set(verbs)\n",
    "\n",
    "# ROOT, conj, xcomp (open clausal complement, i.e. \"boil\" in \"bring to boil\"), nsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 20000/20000 [10:30<00:00, 31.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# left with 1,233 unique verbs\n",
    "# take a random sample of another 20,000\n",
    "\n",
    "verbs_rand = []\n",
    "for row in tqdm(np.random.choice(dirty['method'].values, 20_000)):\n",
    "    doc = nlp(row)\n",
    "    for tok in doc:\n",
    "        if (tok.pos_ == 'VERB'):\n",
    "            if tok.dep_ == 'ROOT':\n",
    "                verbs_rand.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'conj':\n",
    "                verbs_rand.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'xcomp':\n",
    "                verbs_rand.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'nsubj':\n",
    "                verbs_rand.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'det':\n",
    "                verbs_rand.append(tok.lemma_.lower())\n",
    "\n",
    "verbs_rand = set(verbs_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 20000/20000 [10:20<00:00, 32.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# again\n",
    "\n",
    "verbs_rand_2 = []\n",
    "for row in tqdm(np.random.choice(dirty['method'].values, 20_000)):\n",
    "    doc = nlp(row)\n",
    "    for tok in doc:\n",
    "        if (tok.pos_ == 'VERB'):\n",
    "            if tok.dep_ == 'ROOT':\n",
    "                verbs_rand_2.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'conj':\n",
    "                verbs_rand_2.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'xcomp':\n",
    "                verbs_rand_2.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'nsubj':\n",
    "                verbs_rand_2.append(tok.lemma_.lower())\n",
    "            if tok.dep_ == 'det':\n",
    "                verbs_rand_2.append(tok.lemma_.lower())\n",
    "\n",
    "verbs_rand_2 = set(verbs_rand_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2344"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of joined verb sets\n",
    "len(verbs | verbs_rand | verbs_rand_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join verbs\n",
    "verb_list = list(verbs | verbs_rand | verbs_rand_2)\n",
    "\n",
    "# check for 'flambe'\n",
    "('flambe' in verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick save for the 30+ minutes it took to pull these verbs\n",
    "# and I didn't use a random state\n",
    "# and I need to annotate this in a spreadsheet for time\n",
    "\n",
    "pd.Series(verb_list).to_csv('data/verb_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty Metric plus Manual Annotation\n",
    "\n",
    "Manual annotation done in separate spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.0    123\n4.0     99\n6.0     47\n8.0     19\nName: skill_level, dtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# read in annotated verbs\n",
    "verb_annot = pd.read_csv('data/verb_list_annot.csv')\n",
    "\n",
    "# replace zeros with nulls, drop nulls\n",
    "verb_annot['skill_level'].replace(0, np.nan, inplace=True)\n",
    "verb_annot.dropna(inplace=True)\n",
    "verb_annot['skill_level'] *= 2\n",
    "verb_annot['skill_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_difficulty_weighting(method_string):\n",
    "    '''\n",
    "    matches method string to weighted actions in verb_annot, returns weight value\n",
    "    requires spacy\n",
    "    '''\n",
    "    doc = nlp(method_string)\n",
    "    weights = 0\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == 'VERB' and tok.dep_ == 'ROOT':\n",
    "            if tok.lemma_ in verb_annot['verb'].values:\n",
    "                weights += verb_annot.loc[verb_annot['verb'] == tok.lemma_, 'skill_level'].values[0]\n",
    "    return weights\n",
    "\n",
    "def action_counter(method_string):\n",
    "    '''\n",
    "    hunts for root verb, counts number of actions/moves\n",
    "    requires spacy\n",
    "    '''  \n",
    "    doc = nlp(method_string)\n",
    "    actions = 0\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == 'VERB' and tok.dep_ == 'ROOT':\n",
    "            actions += 1\n",
    "    return actions\n",
    "\n",
    "def difficulty(method_string):\n",
    "    '''\n",
    "    returns actions plus weights for a difficulty rating\n",
    "    '''\n",
    "    return manual_difficulty_weighting(method_string) + action_counter(method_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability Score\n",
    "\n",
    "$Readability = 100 * RLW + ASL$ where $RLW = \\frac{n_{lw}}{n_w}$, $ASL = \\frac{n_w}{n_s}$, $n_{lw} =$ number of words longer than 6 characters, $n_w =$ number of words, and $n_s =$ number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_score(method_string):\n",
    "    '''\n",
    "    returns LIX readability score\n",
    "    '''\n",
    "    doc = nlp(method_string)\n",
    "    words = [tok for tok in doc if tok.is_alpha]\n",
    "    n_words = len(words)\n",
    "    n_long_words = len([word for word in words if len(word) > 6])\n",
    "    n_sents = len(list(doc.sents))\n",
    "    rlw = n_long_words / n_words\n",
    "    asl = n_words / n_sents\n",
    "    readability = 100 * rlw + asl\n",
    "    return round(readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Effort Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effort(method_string, difficulty_weight = 1):\n",
    "    '''\n",
    "    returns engineered effort metric, difficulty weighted by user input\n",
    "    '''\n",
    "    return readability_score(method_string) + difficulty_weight * difficulty(method_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}